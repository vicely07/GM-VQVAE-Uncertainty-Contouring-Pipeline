{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":186,"status":"ok","timestamp":1695403601173,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"},"user_tz":300},"id":"cq-y0u2l6XXS","outputId":"39515e66-9da5-4e25-c94b-b2fc89fc8245"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: nvidia-smi: command not found\n"]}],"source":["import os\n","!nvidia-smi -L"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19107,"status":"ok","timestamp":1695403620274,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"},"user_tz":300},"id":"vCoGBL966nMS","outputId":"d418a440-5577-4c82-bb99-e73170c8f322"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Optional to save generator, checkpoint to your google drive\n","# Prevents data being erased if colab session disconnects\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gEcryQrd6S11","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693695829988,"user_tz":300,"elapsed":519,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"}},"outputId":"9dba7414-1b30-41fe-bf07-19a96d3b6402"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/Dissertation-project/GM-VQVAE'\n","/content\n"]}],"source":["%cd /content/drive/MyDrive/Dissertation-project/GM-VQVAE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-cTY-YYI5yzh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693695831159,"user_tz":300,"elapsed":1176,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"}},"outputId":"b204dfb1-e2d1-4d46-b0d6-3a2d92617cf3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'vq-vae-2-pytorch'...\n","remote: Enumerating objects: 82, done.\u001b[K\n","remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82\u001b[K\n","Receiving objects: 100% (82/82), 6.88 MiB | 16.97 MiB/s, done.\n","Resolving deltas: 100% (37/37), done.\n"]}],"source":["#! git clone https://github.com/rosinality/vq-vae-2-pytorch.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HXOub8lx5hQ5"},"outputs":[],"source":["#! pip install torch"]},{"cell_type":"markdown","metadata":{"id":"N1lpH1wi_XJW"},"source":["## Loading data:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"4kRKiY66_ZZ7","executionInfo":{"status":"ok","timestamp":1695403626203,"user_tz":300,"elapsed":5941,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"}}},"outputs":[],"source":["\"\"\"\n","The following is an import of PyTorch libraries.\n","\"\"\"\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import torch\n","import numpy as np\n","#import nibabel as ni\n","import os, shutil\n","import time\n","import random\n","import pandas as pd\n","import numpy as np\n","import os\n","import cv2\n","import numpy as np\n","import os\n","import cv2\n","from scipy import ndimage\n","import torchvision.transforms.functional as TF\n","import random\n","import matplotlib.pyplot as plt\n","\n","\"\"\"\n","Determine if any GPUs are available\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","source":["## Visualization"],"metadata":{"id":"36OL-nVHU1U0"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"H1G1z6oxyMoT","executionInfo":{"status":"ok","timestamp":1695403626204,"user_tz":300,"elapsed":22,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","def display_image3d(x):\n","  z1,x1,y1 = x.nonzero()\n","  fig = plt.figure()\n","  ax = plt.axes(projection='3d')\n","  ax.scatter(x1, y1, z1, c=z1, cmap='viridis', alpha=0.7)\n","  plt.show()"]},{"cell_type":"markdown","source":["## Data processing"],"metadata":{"id":"NuRSHIpsUyKe"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"RSwdTLM1_a3e","executionInfo":{"status":"ok","timestamp":1695403626206,"user_tz":300,"elapsed":19,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"}}},"outputs":[],"source":["def crop_around_centroid(array, dim1):\n","  i, j = ndimage.center_of_mass(array)\n","  i, j = int(i), int(j)\n","  w = int(dim1/2)\n","  imin = max(0,i-w)\n","  imax = min(array.shape[0],i+w+1)\n","  jmin = max(0,j-w)\n","  jmax = min(array.shape[1],j+w+1)\n","  crop =  array[imin:imax,jmin:jmax]\n","  return crop\n","\n","def standard_resize2d(image, dim):\n","  resize_x,  resize_y = dim[0], dim[1]\n","  img_sm = cv2.resize(image, (resize_x, resize_y), interpolation=cv2.INTER_CUBIC)\n","\n","  return img_sm\n","\n","\n","def split_train_test(dir, ratio_test=0.15):\n","    if not os.path.exists(os.path.join(dir, \"train\")): os.mkdir(os.path.join(dir, \"train\"))\n","    if not os.path.exists(os.path.join(dir, \"test\")): os.mkdir(os.path.join(dir, \"test\"))\n","\n","    images_list = [i for i in os.listdir(dir) if i.endswith(\".nii\")]\n","\n","    random.shuffle(images_list)\n","    threshold = int(len(images_list)*ratio_test)\n","    train_list = images_list[:-threshold]\n","    test_list = images_list[-threshold:]\n","\n","    for i in train_list:\n","        shutil.move(os.path.join(dir, i), os.path.join(dir, \"train\", i))\n","    for i in test_list:\n","        shutil.move(os.path.join(dir, i), os.path.join(dir, \"test\", i))\n","\n","def save_data_to_csv(dir, z):\n","    pd.DataFrame(z).to_csv(dir, header=None, index=False)\n","\n","def load_data_images(path, batch_size):\n","    filenames = [i for i in os.listdir(path) if i.endswith(\".npy\")] #and i.startswith(\"norm_023_S_0030\")\n","    random.shuffle(filenames, random.random)\n","    n = 0\n","    while n < len(filenames):\n","        batch_image = []\n","        for i in range(n, n + batch_size):\n","            #print(filenames[i])\n","            if i >= len(filenames):\n","                ##n = i\n","                break\n","\n","            image = np.load(os.path.join(path, filenames[i]), allow_pickle=True)#[1, ...]\n","            image = np.where(image >= 1e-3, image, 0.0)\n","            image = crop_around_centroid(image, dim1=240)\n","            image = np.pad(image, ((1,0), (1,0)), \"constant\", constant_values=0)\n","            dim = (256,256)\n","            image = torch.Tensor(standard_resize2d(image, dim))\n","            #image = random_rotate_transforms(image)\n","            image = torch.reshape(image, (1,1, 256, 256))\n","            image = (image - torch.min(image)) / (torch.max(image) - torch.min(image))\n","            image = torch.where(image >= 0.1, image, 0.0)\n","            batch_image.append(image)\n","\n","        n += batch_size\n","        batch_image = torch.cat(batch_image, axis=0)\n","        yield batch_image"]},{"cell_type":"markdown","metadata":{"id":"4xi9oQAplTj6"},"source":["## Util Functions"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"sVm33nwg9ltR","executionInfo":{"status":"ok","timestamp":1695403626207,"user_tz":300,"elapsed":15,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"}}},"outputs":[],"source":["import math\n","import pickle\n","\n","import torch\n","from torch import distributed as dist\n","from torch.utils import data\n","\n","\n","LOCAL_PROCESS_GROUP = None\n","\n","\n","def is_primary():\n","    return get_rank() == 0\n","\n","\n","def get_rank():\n","    if not dist.is_available():\n","        return 0\n","\n","    if not dist.is_initialized():\n","        return 0\n","\n","    return dist.get_rank()\n","\n","\n","def get_local_rank():\n","    if not dist.is_available():\n","        return 0\n","\n","    if not dist.is_initialized():\n","        return 0\n","\n","    if LOCAL_PROCESS_GROUP is None:\n","        raise ValueError(\"tensorfn.distributed.LOCAL_PROCESS_GROUP is None\")\n","\n","    return dist.get_rank(group=LOCAL_PROCESS_GROUP)\n","\n","\n","def synchronize():\n","    if not dist.is_available():\n","        return\n","\n","    if not dist.is_initialized():\n","        return\n","\n","    world_size = dist.get_world_size()\n","\n","    if world_size == 1:\n","        return\n","\n","    dist.barrier()\n","\n","\n","def get_world_size():\n","    if not dist.is_available():\n","        return 1\n","\n","    if not dist.is_initialized():\n","        return 1\n","\n","    return dist.get_world_size()\n","\n","\n","def all_reduce(tensor, op=dist.ReduceOp.SUM):\n","    world_size = get_world_size()\n","\n","    if world_size == 1:\n","        return tensor\n","\n","    dist.all_reduce(tensor, op=op)\n","\n","    return tensor\n","\n","\n","def all_gather(data):\n","    world_size = get_world_size()\n","\n","    if world_size == 1:\n","        return [data]\n","\n","    buffer = pickle.dumps(data)\n","    storage = torch.ByteStorage.from_buffer(buffer)\n","    tensor = torch.ByteTensor(storage).to(\"cuda\")\n","\n","    local_size = torch.IntTensor([tensor.numel()]).to(\"cuda\")\n","    size_list = [torch.IntTensor([1]).to(\"cuda\") for _ in range(world_size)]\n","    dist.all_gather(size_list, local_size)\n","    size_list = [int(size.item()) for size in size_list]\n","    max_size = max(size_list)\n","\n","    tensor_list = []\n","    for _ in size_list:\n","        tensor_list.append(torch.ByteTensor(size=(max_size,)).to(\"cuda\"))\n","\n","    if local_size != max_size:\n","        padding = torch.ByteTensor(size=(max_size - local_size,)).to(\"cuda\")\n","        tensor = torch.cat((tensor, padding), 0)\n","\n","    dist.all_gather(tensor_list, tensor)\n","\n","    data_list = []\n","\n","    for size, tensor in zip(size_list, tensor_list):\n","        buffer = tensor.cpu().numpy().tobytes()[:size]\n","        data_list.append(pickle.loads(buffer))\n","\n","    return data_list\n","\n","\n","def reduce_dict(input_dict, average=True):\n","    world_size = get_world_size()\n","\n","    if world_size < 2:\n","        return input_dict\n","\n","    with torch.no_grad():\n","        keys = []\n","        values = []\n","\n","        for k in sorted(input_dict.keys()):\n","            keys.append(k)\n","            values.append(input_dict[k])\n","\n","        values = torch.stack(values, 0)\n","        dist.reduce(values, dst=0)\n","\n","        if dist.get_rank() == 0 and average:\n","            values /= world_size\n","\n","        reduced_dict = {k: v for k, v in zip(keys, values)}\n","\n","    return reduced_dict\n","\n","\n","def data_sampler(dataset, shuffle, distributed):\n","    if distributed:\n","        return data.distributed.DistributedSampler(dataset, shuffle=shuffle)\n","\n","    if shuffle:\n","        return data.RandomSampler(dataset)\n","\n","    else:\n","        return data.SequentialSampler(dataset)"]},{"cell_type":"markdown","source":["## GMM component"],"metadata":{"id":"lRIl5Sm5YJX2"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"wtHCzjvmH26E","executionInfo":{"status":"ok","timestamp":1695403626208,"user_tz":300,"elapsed":13,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"}}},"outputs":[],"source":["\"\"\"\n","The following is an import of PyTorch libraries.\n","\"\"\"\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import torch\n","import numpy as np\n","#import nibabel as ni\n","import os, shutil\n","import time\n","import random\n","import pandas as pd\n","import numpy as np\n","import os\n","import cv2\n","import numpy as np\n","import os\n","import cv2\n","from scipy import ndimage\n","import torchvision.transforms.functional as TF\n","import random\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.init as init\n","from torch import nn\n","from torch.nn import functional as F\n","#from networks.Layers import *\n","\n","\n","\"\"\"\n","Determine if any GPUs are available\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","# Flatten layer\n","class Flatten(nn.Module):\n","  def forward(self, x):\n","    return x.view(x.size(0), -1)\n","\n","# Reshape layer\n","class Reshape(nn.Module):\n","  def __init__(self, outer_shape):\n","    super(Reshape, self).__init__()\n","    self.outer_shape = outer_shape\n","  def forward(self, x):\n","    return x.view(x.size(0), *self.outer_shape)\n","\n","# Sample from the Gumbel-Softmax distribution and optionally discretize.\n","class GumbelSoftmax(nn.Module):\n","\n","  def __init__(self, f_dim, c_dim):\n","    super(GumbelSoftmax, self).__init__()\n","    self.logits = nn.Linear(f_dim, c_dim)\n","    self.f_dim = f_dim\n","    self.c_dim = c_dim\n","\n","  def sample_gumbel(self, shape, is_cuda=False, eps=1e-20):\n","    U = torch.rand(shape)\n","    if is_cuda:\n","      U = U.to(device)\n","    return -torch.log(-torch.log(U + eps) + eps)\n","\n","  def gumbel_softmax_sample(self, logits, temperature):\n","    y = logits + self.sample_gumbel(logits.size(), logits.is_cuda)\n","    return F.softmax(y / temperature, dim=-1)\n","\n","  def gumbel_softmax(self, logits, temperature, hard=False):\n","    \"\"\"\n","    ST-gumple-softmax\n","    input: [*, n_class]\n","    return: flatten --> [*, n_class] an one-hot vector\n","    \"\"\"\n","    #categorical_dim = 10\n","    y = self.gumbel_softmax_sample(logits, temperature)\n","\n","    if not hard:\n","        return y\n","\n","    shape = y.size()\n","    _, ind = y.max(dim=-1)\n","    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n","    y_hard.scatter_(1, ind.view(-1, 1), 1)\n","    y_hard = y_hard.view(*shape)\n","    # Set gradients w.r.t. y_hard gradients w.r.t. y\n","    y_hard = (y_hard - y).detach() + y\n","    return y_hard\n","\n","  def forward(self, x, temperature=1.0, hard=False):\n","    logits = self.logits(x).view(-1, self.c_dim)\n","    prob = F.softmax(logits, dim=-1)\n","    y = self.gumbel_softmax(logits, temperature, hard)\n","    return logits, prob, y\n","\n","# Sample from a Gaussian distribution\n","class Gaussian(nn.Module):\n","  def __init__(self, in_dim, z_dim):\n","    super(Gaussian, self).__init__()\n","    self.mu = nn.Linear(in_dim, z_dim)\n","    self.var = nn.Linear(in_dim, z_dim)\n","\n","  def reparameterize(self, mu, var):\n","    std = torch.sqrt(var + 1e-10)\n","    noise = torch.randn_like(std)\n","    z = mu + noise * std\n","    return z\n","\n","  def forward(self, x):\n","    mu = self.mu(x)\n","    var = F.softplus(self.var(x))\n","    z = self.reparameterize(mu, var)\n","    return mu, var, z\n","\n","# Inference Network\n","class InferenceNet(nn.Module):\n","  def __init__(self, x_dim, z_dim, y_dim):\n","    super(InferenceNet, self).__init__()\n","\n","    # q(y|x)\n","    self.inference_qyx = torch.nn.ModuleList([\n","        nn.Linear(x_dim, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 512),\n","        nn.ReLU(),\n","        GumbelSoftmax(512, y_dim)\n","    ])\n","\n","    # q(z|y,x)\n","    self.inference_qzyx = torch.nn.ModuleList([\n","        nn.Linear(x_dim + y_dim, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 512),\n","        nn.ReLU(),\n","        Gaussian(512, z_dim)\n","    ])\n","\n","  # q(y|x)\n","  def qyx(self, x, temperature, hard):\n","    num_layers = len(self.inference_qyx)\n","    for i, layer in enumerate(self.inference_qyx):\n","      if i == num_layers - 1:\n","        #last layer is gumbel softmax\n","        x = layer(x, temperature, hard)\n","      else:\n","        x = layer(x)\n","    return x\n","\n","  # q(z|x,y)\n","  def qzxy(self, x, y):\n","    concat = torch.cat((x, y), dim=1)\n","    for layer in self.inference_qzyx:\n","      concat = layer(concat)\n","    return concat\n","\n","  def forward(self, x, temperature=1.0, hard=0):\n","    #x = Flatten(x)\n","\n","    # q(y|x)\n","    logits, prob, y = self.qyx(x, temperature, hard)\n","\n","    # q(z|x,y)\n","    mu, var, z = self.qzxy(x, y)\n","\n","    output = {'mean': mu, 'var': var, 'gaussian': z,\n","              'logits': logits, 'prob_cat': prob, 'categorical': y}\n","    return output\n","\n","\n","# Generative Network\n","class GenerativeNet(nn.Module):\n","  def __init__(self, x_dim, z_dim, y_dim):\n","    super(GenerativeNet, self).__init__()\n","\n","    # p(z|y)\n","    self.y_mu = nn.Linear(y_dim, z_dim)\n","    self.y_var = nn.Linear(y_dim, z_dim)\n","    # p(x|z)\n","    self.generative_pxz = torch.nn.ModuleList([\n","        nn.Linear(z_dim, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, x_dim),\n","        torch.nn.Sigmoid()\n","    ])\n","\n","  # p(z|y)\n","  def pzy(self, y):\n","    y_mu = self.y_mu(y)\n","    y_var = F.softplus(self.y_var(y))\n","    return y_mu, y_var\n","\n","  # p(x|z)\n","  def pxz(self, z):\n","    for layer in self.generative_pxz:\n","      z = layer(z)\n","    return z\n","\n","  def forward(self, z, y):\n","    # p(z|y)\n","    y_mu, y_var = self.pzy(y)\n","\n","    # p(x|z)\n","    x_rec = self.pxz(z)\n","\n","    output = {'y_mean': y_mu, 'y_var': y_var, 'x_rec': x_rec}\n","    return output\n","\n","\n","# GMVAE Network\n","class GMVAENet(nn.Module):\n","  def __init__(self, x_dim, z_dim, y_dim):\n","    super(GMVAENet, self).__init__()\n","    self.inference = InferenceNet(x_dim, z_dim, y_dim)\n","    self.generative = GenerativeNet(x_dim, z_dim, y_dim)\n","\n","    # weight initialization\n","    for m in self.modules():\n","      if type(m) == nn.Linear or type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:\n","        torch.nn.init.xavier_normal_(m.weight)\n","        if m.bias.data is not None:\n","          init.constant_(m.bias, 0)\n","\n","  def forward(self, x, temperature=1.0, hard=0):\n","    x = x.view(x.size(0), -1)\n","    out_inf = self.inference(x, temperature, hard)\n","    z, y = out_inf['gaussian'], out_inf['categorical']\n","    #out_gen = self.generative(z, y)\n","\n","    # merge output\n","    #output = out_inf\n","    #for key, value in out_gen.items():\n","      #output[key] = value\n","\n","    return z #output"]},{"cell_type":"markdown","source":["## GM-VQVAE model"],"metadata":{"id":"0VtK6y75ULXF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMbdDSShA_fv"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","import distributed as dist_fn\n","\n","\n","# Copyright 2018 The Sonnet Authors. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#    http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ============================================================================\n","\n","\n","# Borrowed from https://github.com/deepmind/sonnet and ported it to PyTorch\n","\n","\n","class Quantize(nn.Module):\n","    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n","        super().__init__()\n","\n","        self.dim = dim\n","        self.n_embed = n_embed\n","        self.decay = decay\n","        self.eps = eps\n","\n","        embed = torch.randn(dim, n_embed)\n","        self.register_buffer(\"embed\", embed)\n","        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n","        self.register_buffer(\"embed_avg\", embed.clone())\n","\n","    def forward(self, input):\n","        flatten = input.reshape(-1, self.dim)\n","        dist = (\n","            flatten.pow(2).sum(1, keepdim=True)\n","            - 2 * flatten @ self.embed\n","            + self.embed.pow(2).sum(0, keepdim=True)\n","        )\n","        _, embed_ind = (-dist).max(1)\n","        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n","        embed_ind = embed_ind.view(*input.shape[:-1])\n","        quantize = self.embed_code(embed_ind)\n","\n","        if self.training:\n","            embed_onehot_sum = embed_onehot.sum(0)\n","            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n","\n","            all_reduce(embed_onehot_sum)\n","            all_reduce(embed_sum)\n","\n","            self.cluster_size.data.mul_(self.decay).add_(\n","                embed_onehot_sum, alpha=1 - self.decay\n","            )\n","            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n","            n = self.cluster_size.sum()\n","            cluster_size = (\n","                (self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n\n","            )\n","            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n","            self.embed.data.copy_(embed_normalized)\n","\n","        diff = (quantize.detach() - input).pow(2).mean()\n","        quantize = input + (quantize - input).detach()\n","\n","        return quantize, diff, embed_ind\n","\n","    def embed_code(self, embed_id):\n","        return F.embedding(embed_id, self.embed.transpose(0, 1))\n","\n","\n","class ResBlock(nn.Module):\n","    def __init__(self, in_channel, channel):\n","        super().__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.ReLU(),\n","            nn.Conv2d(in_channel, channel, 3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(channel, in_channel, 1),\n","        )\n","\n","    def forward(self, input):\n","        out = self.conv(input)\n","        out += input\n","\n","        return out\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, in_channel, channel, n_res_block, n_res_channel, stride):\n","        super().__init__()\n","\n","        if stride == 4:\n","            blocks = [\n","                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(channel // 2, channel, 4, stride=2, padding=1),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(channel, channel, 3, padding=1),\n","            ]\n","\n","        elif stride == 2:\n","            blocks = [\n","                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(channel // 2, channel, 3, padding=1),\n","            ]\n","\n","        for i in range(n_res_block):\n","            blocks.append(ResBlock(channel, n_res_channel))\n","\n","        blocks.append(nn.ReLU(inplace=True))\n","\n","        self.blocks = nn.Sequential(*blocks)\n","\n","    def forward(self, input):\n","        return self.blocks(input)\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(\n","        self, in_channel, out_channel, channel, n_res_block, n_res_channel, stride\n","    ):\n","        super().__init__()\n","\n","        blocks = [nn.Conv2d(in_channel, channel, 3, padding=1)]\n","\n","        for i in range(n_res_block):\n","            blocks.append(ResBlock(channel, n_res_channel))\n","\n","        blocks.append(nn.ReLU(inplace=True))\n","\n","        if stride == 4:\n","            blocks.extend(\n","                [\n","                    nn.ConvTranspose2d(channel, channel // 2, 4, stride=2, padding=1),\n","                    nn.ReLU(inplace=True),\n","                    nn.ConvTranspose2d(\n","                        channel // 2, out_channel, 4, stride=2, padding=1\n","                    ),\n","                ]\n","            )\n","\n","        elif stride == 2:\n","            blocks.append(\n","                nn.ConvTranspose2d(channel, out_channel, 4, stride=2, padding=1)\n","            )\n","\n","        self.blocks = nn.Sequential(*blocks)\n","\n","    def forward(self, input):\n","        return self.blocks(input)\n","\n","\n","class VQVAE(nn.Module):\n","    def __init__(\n","        self,\n","        in_channel=1,\n","        channel=128,\n","        n_res_block=2,\n","        n_res_channel=32,\n","        embed_dim=64,\n","        n_embed=512,\n","        decay=0.99,\n","    ):\n","        super().__init__()\n","        self.enc_b = Encoder(in_channel, channel, n_res_block, n_res_channel, stride=4)\n","        self.enc_t = Encoder(channel, channel, n_res_block, n_res_channel, stride=2)\n","        self.quantize_conv_t = nn.Conv2d(channel, embed_dim, 1)\n","        self.quantize_t = Quantize(embed_dim, n_embed)\n","        self.dec_t = Decoder(\n","            embed_dim, embed_dim, channel, n_res_block, n_res_channel, stride=2\n","        )\n","        self.quantize_conv_b = nn.Conv2d(embed_dim + channel, embed_dim, 1)\n","        self.quantize_b = Quantize(embed_dim, n_embed)\n","        self.upsample_t = nn.ConvTranspose2d(\n","            embed_dim, embed_dim, 4, stride=2, padding=1\n","        )\n","        self.dec = Decoder(\n","            embed_dim + embed_dim,\n","            in_channel,\n","            channel,\n","            n_res_block,\n","            n_res_channel,\n","            stride=4,\n","        )\n","\n","\n","    def forward(self, input):\n","\n","        quant_t, quant_b, diff, _, _ = self.encode(input)\n","        #GMM  representation\n","        quant_t_scale, quant_b_scale = self.gmm_represent(quant_t, gaussian_size=quant_t.shape[-1]), self.gmm_represent(quant_b, gaussian_size=quant_b.shape[-1])\n","        #quant_t_scale, quant_b_scale = torch.mean(torch.flatten(quant_t_scale.permute(0, 2, 1, 3), start_dim=2), dim=2), torch.mean(torch.flatten(quant_b_scale.permute(0, 2, 1, 3), start_dim=2), dim=2)\n","        dec = self.decode(quant_t * quant_t_scale, quant_b * quant_b_scale)\n","\n","        return dec, diff\n","\n","    def encode(self, input):\n","        enc_b = self.enc_b(input)\n","        enc_t = self.enc_t(enc_b)\n","\n","        quant_t = self.quantize_conv_t(enc_t).permute(0, 2, 3, 1)\n","        quant_t, diff_t, id_t = self.quantize_t(quant_t)\n","        quant_t = quant_t.permute(0, 3, 1, 2)\n","        diff_t = diff_t.unsqueeze(0)\n","\n","        dec_t = self.dec_t(quant_t)\n","        enc_b = torch.cat([dec_t, enc_b], 1)\n","\n","        quant_b = self.quantize_conv_b(enc_b).permute(0, 2, 3, 1)\n","        quant_b, diff_b, id_b = self.quantize_b(quant_b)\n","        quant_b = quant_b.permute(0, 3, 1, 2)\n","        diff_b = diff_b.unsqueeze(0)\n","\n","        return quant_t, quant_b, diff_t + diff_b, id_t, id_b\n","\n","    def decode(self, quant_t, quant_b):\n","        upsample_t = self.upsample_t(quant_t)\n","        quant = torch.cat([upsample_t, quant_b], 1)\n","        dec = self.dec(quant)\n","\n","        return dec\n","\n","    def decode_code(self, code_t, code_b):\n","        quant_t = self.quantize_t.embed_code(code_t)\n","        quant_t = quant_t.permute(0, 3, 1, 2)\n","        quant_b = self.quantize_b.embed_code(code_b)\n","        quant_b = quant_b.permute(0, 3, 1, 2)\n","\n","        dec = self.decode(quant_t, quant_b)\n","\n","        return dec\n","\n","    def gmm_represent(self, quant, gaussian_size = 64):\n","      # GMM for quant_t\n","      input_size = quant.shape[1] * quant.shape[2] * quant.shape[3]\n","      num_classes = 1\n","      gmvae = GMVAENet(input_size, gaussian_size, num_classes).to(device)\n","      quant_out = quant.view(quant.shape[0], 1, quant.shape[1], -1)\n","      quant_out = gmvae(quant_out)\n","      #quant_out = 2 * ((quant_out - torch.min(quant_out))/(torch.max(quant_out)-torch.min(quant_out))) - 1\n","      return torch.sigmoid(quant_out)"]},{"cell_type":"markdown","source":["## Loss function"],"metadata":{"id":"hqIMIdTtUHFV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"42rLHeaXltv_"},"outputs":[],"source":["class KLDivergence(nn.Module):\n","    \"KL divergence between the estimated normal distribution and a prior distribution\"\n","    def __init__(self):\n","        super(KLDivergence, self).__init__()\n","        \"\"\"\n","        N :  the index N spans all dimensions of input\n","        N = H x W x D\n","        \"\"\"\n","        self.N = 80*80\n","    def forward(self, z_mean, z_sigma):\n","        z_var = z_sigma * 2\n","        #return (1/self.N) * ( (z_mean**2 + z_var**2 - z_log_var**2 - 1).sum() )\n","        return 0.5 * ((z_mean**2 + z_var.exp() - z_var - 1).sum())\n","\n","class L1Loss(nn.Module):\n","\n","    \"Measuring the `Euclidian distance` between prediction and ground truh using `L1 Norm`\"\n","    def __init__(self):\n","        super(L1Loss, self).__init__()\n","\n","    def forward(self, x, y):\n","        N = y.shape[0]*y.shape[1]*y.shape[2]*y.shape[3]*y.shape[4]\n","        return  ( (x - y).abs()).sum() / N\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","\n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = F.sigmoid(inputs)\n","\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","\n","        return 1 - dice\n","\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d):\n","        torch.nn.init.xavier_uniform(m.weight.data)"]},{"cell_type":"markdown","source":["# Training Scripts"],"metadata":{"id":"twrLeZHPU_Sh"}},{"cell_type":"markdown","source":["## Joint training"],"metadata":{"id":"xoR5wl7aR7F1"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":581},"executionInfo":{"elapsed":11827,"status":"error","timestamp":1694290102468,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"},"user_tz":300},"id":"ditRGnuSuZII","outputId":"a6ec0383-3022-4966-f2ab-ede0c80cfd36"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-6c108b0efd2c>:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  torch.nn.init.xavier_uniform(m.weight.data)\n"]},{"output_type":"stream","name":"stdout","text":[" GPU is activated\n","Number of train fm images:  5000\n","Number of val fm images:  775\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/100 [00:00<?, ?it/s]\n","0it [00:00, ?it/s]\u001b[A<ipython-input-5-a8050d94af31>:40: DeprecationWarning: The *random* parameter to shuffle() has been deprecated\n","since Python 3.9 and will be removed in a subsequent version.\n","  random.shuffle(filenames, random.random)\n","0it [00:11, ?it/s]\n","  0%|          | 0/100 [00:11<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-2c75c5ad995a>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m#train_batch_images = torch.concat([train_batch_images, train_batch_images, train_batch_images], dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtrain_batch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mtrain_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_latent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mtrain_recon_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-73dc5fa88b51>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mquant_t_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_b_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgmm_represent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaussian_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgmm_represent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaussian_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;31m#quant_t_scale, quant_b_scale = torch.mean(torch.flatten(quant_t_scale.permute(0, 2, 1, 3), start_dim=2), dim=2), torch.mean(torch.flatten(quant_b_scale.permute(0, 2, 1, 3), start_dim=2), dim=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_t\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mquant_t_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_b\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mquant_b_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (10) at non-singleton dimension 2"]}],"source":["\"\"\"\n","Initialize Hyperparameters\n","\"\"\"\n","#import nibabel as ni\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os, glob\n","import torch\n","import csv\n","from tqdm import tqdm\n","##---------Settings--------------------------\n","batch_size = 10\n","lrate = 0.001\n","epochs = 100\n","weight_decay = 1e-6 #5e-7\n","\n","##############\n","train_path_data = \"/content/drive/MyDrive/Dissertation-project/Data/2D-featuremap/all/train/sample\"\n","val_path_data =\"/content/drive/MyDrive/Dissertation-project/Data/2D-featuremap/all/val/\"\n","path2save = \"/content/drive/MyDrive/Dissertation-project/checkpoint/2d_vae/vqvae/vqvae_n32/joint/model_vae_epoch_{}.pt\"\n","dir_info = '/content/drive/MyDrive/Dissertation-project/infor'\n","#f = open(os.path.join(dir_info,'model_vae_t1_rectum_2d.csv'),'w',newline='')\n","\n","####################\n","verbose = True\n","log = print if verbose else lambda *x, **i: None\n","np.random.seed(10)\n","torch.manual_seed(10)\n","###################\n","criterion = nn.MSELoss()\n","\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\" GPU is activated\" if device else \" CPU is activated\")\n","train_no_images = len(glob.glob(train_path_data + \"/*.npy\"))\n","val_no_images = len(glob.glob(val_path_data + \"/*.npy\"))\n","print(\"Number of train fm images: \", train_no_images)\n","print(\"Number of val fm images: \", val_no_images)\n","\n","if __name__==\"__main__\":\n","    vae_model = VQVAE()\n","    vae_model.to(device)\n","    optimizer = torch.optim.Adam(vae_model.parameters(), lr=lrate, weight_decay=weight_decay)\n","    vae_model.apply(weights_init)\n","    min_valid_loss = np.inf\n","    train_batch_avg_total_loss_list = []\n","    val_batch_avg_total_loss_list = []\n","    beta = 0.25\n","\n","    for epoch in tqdm(range(epochs)):\n","        # training phrase\n","        train_loss_rec_batch, train_loss_KL_batch, train_total_loss_batch = 0, 0, 0\n","        train_loss_rec_epoch, train_loss_KL_epoch, train_total_loss_epoch = 0, 0, 0\n","\n","        vae_model.train()\n","        for train_batch_images in tqdm(load_data_images(train_path_data, batch_size)):\n","            optimizer.zero_grad()\n","            #train_batch_images = torch.concat([train_batch_images, train_batch_images, train_batch_images], dim=1)\n","            train_batch_images = train_batch_images.to(device)\n","            train_out, train_latent_loss = vae_model(train_batch_images)\n","\n","            train_recon_loss = criterion(train_out, train_batch_images)\n","            train_latent_loss = train_latent_loss.mean()\n","\n","            train_total_loss_batch =  train_recon_loss + beta * train_latent_loss\n","\n","            # Optimize\n","            train_total_loss_batch.backward()\n","            optimizer.step()\n","\n","            train_total_loss_epoch += train_total_loss_batch.item() * train_batch_images.shape[0]\n","\n","        train_log_info = (epoch + 1, epochs, train_loss_rec_epoch/train_no_images, train_loss_KL_epoch/train_no_images, train_total_loss_epoch/train_no_images)\n","        log('%d/%d  Train: Reconstruction Loss %.3f| KL Loss %.3f | Total Loss %.3f'% train_log_info)\n","        train_batch_avg_total_loss_list.append(train_total_loss_epoch/train_no_images)\n","\n","\n","        # validation phrase\n","        val_loss_rec_batch, val_loss_KL_batch, val_total_loss_batch = 0, 0, 0\n","        val_loss_rec_epoch, val_loss_KL_epoch, val_total_loss_epoch = 0, 0, 0\n","\n","\n","        vae_model.eval()\n","        for val_batch_images in tqdm(load_data_images(val_path_data, batch_size)):\n","            optimizer.zero_grad()\n","            #val_batch_images = torch.concat([val_batch_images, val_batch_images, val_batch_images], dim=1)\n","            val_batch_images = val_batch_images.to(device)\n","\n","            val_batch_images = val_batch_images.to(device)\n","            val_out, val_latent_loss = vae_model(val_batch_images)\n","            val_recon_loss = criterion(val_out, val_batch_images)\n","            val_latent_loss = val_latent_loss.mean()\n","\n","            val_total_loss_batch =  val_recon_loss + beta * val_latent_loss\n","\n","            # Optimize\n","            val_total_loss_batch.backward()\n","            optimizer.step()\n","            val_total_loss_epoch += val_total_loss_batch.item() * val_batch_images.shape[0]\n","\n","\n","        val_log_info = (epoch + 1, epochs,  val_loss_rec_epoch/val_no_images, val_loss_KL_epoch/val_no_images, val_total_loss_epoch/val_no_images)\n","        log('%d/%d  Validation: Reconstruction Loss %.3f| KL Loss %.3f | Total Loss %.3f'% val_log_info)\n","        val_batch_avg_total_loss_list.append(val_total_loss_epoch/val_no_images)\n","        epoch_log_df = pd.DataFrame({\"train_loss\": train_batch_avg_total_loss_list, \"val_loss\": val_batch_avg_total_loss_list})\n","        print(epoch_log_df)\n","        epoch_log_df.to_csv(os.path.join(dir_info,'model_vae_t1_prostate_2d_16.csv'))\n","        if min_valid_loss > val_total_loss_epoch/val_no_images:\n","          print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{val_total_loss_epoch/val_no_images:.6f}) \\t Saving The Model')\n","          min_valid_loss = val_total_loss_epoch/val_no_images\n","          # Saving State Dict\n","\n","        torch.save(vae_model, path2save.format(epoch+1))\n","        # write csv\n","        #writer = csv.writer(f)\n","        #writer.writerow([epoch + 1, '{:04f}'.format(val_total_loss_epoch/train_no_images), '{:04f}'.format(val_total_loss_epoch/val_no_images)])\n","    #f.close()"]},{"cell_type":"markdown","source":["## Prostate training"],"metadata":{"id":"Ml6d9Kq7R1_0"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":668},"id":"wn3NH22XUbyK","executionInfo":{"status":"error","timestamp":1693281110900,"user_tz":300,"elapsed":216339,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"}},"outputId":"75a34ed8-fc85-4de5-a161-cb9e677ddd36"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-1eb2d7fd442d>:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  torch.nn.init.xavier_uniform(m.weight.data)\n"]},{"output_type":"stream","name":"stdout","text":[" GPU is activated\n","Number of train fm images:  5000\n","Number of val fm images:  916\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/100 [00:00<?, ?it/s]\n","0it [00:00, ?it/s]\u001b[A<ipython-input-6-b70cece9af57>:40: DeprecationWarning: The *random* parameter to shuffle() has been deprecated\n","since Python 3.9 and will be removed in a subsequent version.\n","  random.shuffle(filenames, random.random)\n"]},{"output_type":"stream","name":"stdout","text":["Encoding bottom\n","torch.Size([312, 128, 64, 64])\n","Encoding top\n","torch.Size([312, 128, 32, 32])\n"]},{"output_type":"stream","name":"stderr","text":["0it [03:36, ?it/s]\n","  0%|          | 0/100 [03:36<?, ?it/s]\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-66d0fa23dddf>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             '''\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mtrain_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_latent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             '''\n\u001b[1;32m     73\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-ae9f14c4038c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mquant_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;31m#GMM  representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-ae9f14c4038c>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mquant_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_conv_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mquant_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mquant_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mdiff_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiff_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-ae9f14c4038c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n\u001b[1;32m     48\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0membed_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0membed_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_ind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mquantize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.88 GiB (GPU 0; 14.75 GiB total capacity; 9.72 GiB already allocated; 3.50 GiB free; 10.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["\"\"\"\n","Initialize Hyperparameters\n","\"\"\"\n","#import nibabel as ni\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os, glob\n","import torch\n","import csv\n","from tqdm import tqdm\n","##---------Settings--------------------------\n","batch_size = 312\n","lrate = 0.001\n","epochs = 100\n","weight_decay = 1e-6 #5e-7\n","\n","##############\n","train_path_data = \"/content/drive/MyDrive/VAE_GMM/2D-featuremap/Prostate/Prostate_train/sample\"\n","val_path_data =\"/content/drive/MyDrive/VAE_GMM/2D-featuremap/Prostate/Prostate_val\"\n","path2save = \"/content/drive/MyDrive/VAE_GMM/VQ-VAE2/vqvae2/vqvae2_checkpoint/prostate/model_vae_epoch_{}.pt\"\n","dir_info = '/content/drive/MyDrive/VAE_GMM/infor'\n","#f = open(os.path.join(dir_info,'model_vae_t1_rectum_2d.csv'),'w',newline='')\n","\n","####################\n","verbose = True\n","log = print if verbose else lambda *x, **i: None\n","np.random.seed(10)\n","torch.manual_seed(10)\n","###################\n","criterion = nn.MSELoss()\n","\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\" GPU is activated\" if device else \" CPU is activated\")\n","train_no_images = len(glob.glob(train_path_data + \"/*.npy\"))\n","val_no_images = len(glob.glob(val_path_data + \"/*.npy\"))\n","print(\"Number of train fm images: \", train_no_images)\n","print(\"Number of val fm images: \", val_no_images)\n","\n","if __name__==\"__main__\":\n","    vae_model = VQVAE()\n","    #ckp_path = \"/content/drive/MyDrive/VAE_GMM/VQ-VAE2/vqvae2/vqvae_560.pt\"\n","    #checkpoint =  torch.load(ckp_path)\n","    #vae_model.load_state_dict(checkpoint)\n","    vae_model.to(device)\n","    optimizer = torch.optim.Adam(vae_model.parameters(), lr=lrate, weight_decay=weight_decay)\n","\n","    vae_model.apply(weights_init)\n","    #log(vae_model)\n","    #optimizer = torch.optim.Adam(vae_model.parameters(), lr=lrate, weight_decay=weight_decay)\n","    min_valid_loss = np.inf\n","    #model, optimizer, start_epoch = load_ckp(ckp_path, vae_model, optimizer)\n","    train_batch_avg_total_loss_list = []\n","    val_batch_avg_total_loss_list = []\n","    beta = 0.25\n","\n","    for epoch in tqdm(range(epochs)):\n","        # training phrase\n","        train_loss_rec_batch, train_loss_KL_batch, train_total_loss_batch = 0, 0, 0\n","        train_loss_rec_epoch, train_loss_KL_epoch, train_total_loss_epoch = 0, 0, 0\n","\n","        vae_model.train()\n","        for train_batch_images in tqdm(load_data_images(train_path_data, batch_size)):\n","            optimizer.zero_grad()\n","            #train_batch_images = torch.concat([train_batch_images, train_batch_images, train_batch_images], dim=1)\n","            train_batch_images = train_batch_images.to(device)\n","            '''\n","            plt.imshow(train_batch_images[0, 0, ...].cpu().detach().numpy())\n","            plt.show()\n","            '''\n","            train_out, train_latent_loss = vae_model(train_batch_images)\n","            '''\n","            plt.imshow(train_out[0, 0, ...].cpu().detach().numpy())\n","            plt.show()\n","            '''\n","            train_recon_loss = criterion(train_out, train_batch_images)\n","            train_latent_loss = train_latent_loss.mean()\n","\n","            train_total_loss_batch =  train_recon_loss + beta * train_latent_loss\n","\n","            # Optimize\n","            train_total_loss_batch.backward()\n","            optimizer.step()\n","\n","            train_total_loss_epoch += train_total_loss_batch.item() * train_batch_images.shape[0]\n","\n","        train_log_info = (epoch + 1, epochs, train_loss_rec_epoch/train_no_images, train_loss_KL_epoch/train_no_images, train_total_loss_epoch/train_no_images)\n","        log('%d/%d  Train: Reconstruction Loss %.3f| KL Loss %.3f | Total Loss %.3f'% train_log_info)\n","        train_batch_avg_total_loss_list.append(train_total_loss_epoch/train_no_images)\n","\n","\n","        # validation phrase\n","        val_loss_rec_batch, val_loss_KL_batch, val_total_loss_batch = 0, 0, 0\n","        val_loss_rec_epoch, val_loss_KL_epoch, val_total_loss_epoch = 0, 0, 0\n","\n","\n","        vae_model.eval()\n","        for val_batch_images in tqdm(load_data_images(val_path_data, batch_size)):\n","            optimizer.zero_grad()\n","            #val_batch_images = torch.concat([val_batch_images, val_batch_images, val_batch_images], dim=1)\n","            val_batch_images = val_batch_images.to(device)\n","\n","            val_batch_images = val_batch_images.to(device)\n","            val_out, val_latent_loss = vae_model(val_batch_images)\n","            val_recon_loss = criterion(val_out, val_batch_images)\n","            val_latent_loss = val_latent_loss.mean()\n","\n","            val_total_loss_batch =  val_recon_loss + beta * val_latent_loss\n","\n","            # Optimize\n","            val_total_loss_batch.backward()\n","            optimizer.step()\n","            val_total_loss_epoch += val_total_loss_batch.item() * val_batch_images.shape[0]\n","\n","\n","        val_log_info = (epoch + 1, epochs,  val_loss_rec_epoch/val_no_images, val_loss_KL_epoch/val_no_images, val_total_loss_epoch/val_no_images)\n","        log('%d/%d  Validation: Reconstruction Loss %.3f| KL Loss %.3f | Total Loss %.3f'% val_log_info)\n","        val_batch_avg_total_loss_list.append(val_total_loss_epoch/val_no_images)\n","        epoch_log_df = pd.DataFrame({\"train_loss\": train_batch_avg_total_loss_list, \"val_loss\": val_batch_avg_total_loss_list})\n","        print(epoch_log_df)\n","        epoch_log_df.to_csv(os.path.join(dir_info,'model_vae_t1_prostate_2d_16.csv'))\n","        if min_valid_loss > val_total_loss_epoch/val_no_images:\n","          print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{val_total_loss_epoch/val_no_images:.6f}) \\t Saving The Model')\n","          min_valid_loss = val_total_loss_epoch/val_no_images\n","          # Saving State Dict\n","\n","        torch.save(vae_model, path2save.format(epoch+1))\n","        # write csv\n","        #writer = csv.writer(f)\n","        #writer.writerow([epoch + 1, '{:04f}'.format(val_total_loss_epoch/train_no_images), '{:04f}'.format(val_total_loss_epoch/val_no_images)])\n","    #f.close()"]},{"cell_type":"markdown","source":["## Rectum training"],"metadata":{"id":"fkU9OolGTSBA"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":755},"id":"gJfYB6GbViF1","executionInfo":{"status":"error","timestamp":1693281650941,"user_tz":300,"elapsed":182183,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"}},"outputId":"ee37a116-efd7-4263-87d6-b60589c020ee"},"outputs":[{"output_type":"stream","name":"stdout","text":[" GPU is activated\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-1eb2d7fd442d>:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  torch.nn.init.xavier_uniform(m.weight.data)\n"]},{"output_type":"stream","name":"stdout","text":["Number of train fm images:  5000\n","Number of val fm images:  2127\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/100 [00:00<?, ?it/s]\n","0it [00:00, ?it/s]\u001b[A<ipython-input-6-b70cece9af57>:40: DeprecationWarning: The *random* parameter to shuffle() has been deprecated\n","since Python 3.9 and will be removed in a subsequent version.\n","  random.shuffle(filenames, random.random)\n","\n","1it [00:09,  9.33s/it]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Encoding bottom\n","torch.Size([10, 128, 64, 64])\n","Encoding top\n","torch.Size([10, 128, 32, 32])\n"]},{"output_type":"stream","name":"stderr","text":["\n","2it [00:17,  8.50s/it]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Encoding bottom\n","torch.Size([10, 128, 64, 64])\n","Encoding top\n","torch.Size([10, 128, 32, 32])\n"]},{"output_type":"stream","name":"stderr","text":["2it [00:20, 10.15s/it]\n","  0%|          | 0/100 [00:20<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-b39904a55f86>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mvae_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_batch_images\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m#train_batch_images = torch.concat([train_batch_images, train_batch_images, train_batch_images], dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-b70cece9af57>\u001b[0m in \u001b[0;36mload_data_images\u001b[0;34m(path, batch_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[1, ...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop_around_centroid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0m_ZIP_SUFFIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'PK\\x05\\x06'\u001b[0m \u001b[0;31m# empty zip files start with this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0;31m# If the file size is less than N, we need to make sure not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;31m# to seek past the beginning of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\"\"\"\n","Initialize Hyperparameters\n","\"\"\"\n","#import nibabel as ni\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os, glob\n","import torch\n","import csv\n","from tqdm import tqdm\n","##---------Settings--------------------------\n","batch_size = 10\n","lrate = 0.001\n","epochs = 100\n","weight_decay = 1e-6 #5e-7\n","\n","##############\n","train_path_data = \"/content/drive/MyDrive/VAE_GMM/2D-featuremap/Rectum/Rectum_train/sample\"\n","val_path_data =\"/content/drive/MyDrive/VAE_GMM/2D-featuremap/Rectum/Rectum_val\"\n","path2save = \"/content/drive/MyDrive/VAE_GMM/VQ-VAE2/vqvae2/vqvae2_checkpoint/prostate/model_vae_epoch_{}.pt\"\n","dir_info = '/content/drive/MyDrive/VAE_GMM/infor'\n","#f = open(os.path.join(dir_info,'model_vae_t1_rectum_2d.csv'),'w',newline='')\n","\n","####################\n","verbose = True\n","log = print if verbose else lambda *x, **i: None\n","np.random.seed(10)\n","torch.manual_seed(10)\n","###################\n","criterion = nn.MSELoss()\n","\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\" GPU is activated\" if device else \" CPU is activated\")\n","train_no_images = len(glob.glob(train_path_data + \"/*.npy\"))\n","val_no_images = len(glob.glob(val_path_data + \"/*.npy\"))\n","print(\"Number of train fm images: \", train_no_images)\n","print(\"Number of val fm images: \", val_no_images)\n","\n","if __name__==\"__main__\":\n","    vae_model = VQVAE()\n","    #ckp_path = \"/content/drive/MyDrive/VAE_GMM/VQ-VAE2/vqvae2/vqvae_560.pt\"\n","    #checkpoint =  torch.load(ckp_path)\n","    #vae_model.load_state_dict(checkpoint)\n","    vae_model.to(device)\n","    optimizer = torch.optim.Adam(vae_model.parameters(), lr=lrate, weight_decay=weight_decay)\n","\n","    vae_model.apply(weights_init)\n","    #log(vae_model)\n","    #optimizer = torch.optim.Adam(vae_model.parameters(), lr=lrate, weight_decay=weight_decay)\n","    min_valid_loss = np.inf\n","    #model, optimizer, start_epoch = load_ckp(ckp_path, vae_model, optimizer)\n","    train_batch_avg_total_loss_list = []\n","    val_batch_avg_total_loss_list = []\n","    beta = 0.25\n","\n","    for epoch in tqdm(range(epochs)):\n","        # training phrase\n","        train_loss_rec_batch, train_loss_KL_batch, train_total_loss_batch = 0, 0, 0\n","        train_loss_rec_epoch, train_loss_KL_epoch, train_total_loss_epoch = 0, 0, 0\n","\n","        vae_model.train()\n","        for train_batch_images in tqdm(load_data_images(train_path_data, batch_size)):\n","            optimizer.zero_grad()\n","            #train_batch_images = torch.concat([train_batch_images, train_batch_images, train_batch_images], dim=1)\n","            train_batch_images = train_batch_images.to(device)\n","            '''\n","            plt.imshow(train_batch_images[0, 0, ...].cpu().detach().numpy())\n","            plt.show()\n","            '''\n","            train_out, train_latent_loss = vae_model(train_batch_images)\n","            '''\n","            plt.imshow(train_out[0, 0, ...].cpu().detach().numpy())\n","            plt.show()\n","            '''\n","            train_recon_loss = criterion(train_out, train_batch_images)\n","            train_latent_loss = train_latent_loss.mean()\n","\n","            train_total_loss_batch =  train_recon_loss + beta * train_latent_loss\n","\n","            # Optimize\n","            train_total_loss_batch.backward()\n","            optimizer.step()\n","\n","            train_total_loss_epoch += train_total_loss_batch.item() * train_batch_images.shape[0]\n","\n","        train_log_info = (epoch + 1, epochs, train_loss_rec_epoch/train_no_images, train_loss_KL_epoch/train_no_images, train_total_loss_epoch/train_no_images)\n","        log('%d/%d  Train: Reconstruction Loss %.3f| KL Loss %.3f | Total Loss %.3f'% train_log_info)\n","        train_batch_avg_total_loss_list.append(train_total_loss_epoch/train_no_images)\n","\n","\n","        # validation phrase\n","        val_loss_rec_batch, val_loss_KL_batch, val_total_loss_batch = 0, 0, 0\n","        val_loss_rec_epoch, val_loss_KL_epoch, val_total_loss_epoch = 0, 0, 0\n","\n","\n","        vae_model.eval()\n","        for val_batch_images in tqdm(load_data_images(val_path_data, batch_size)):\n","            optimizer.zero_grad()\n","            #val_batch_images = torch.concat([val_batch_images, val_batch_images, val_batch_images], dim=1)\n","            val_batch_images = val_batch_images.to(device)\n","\n","            val_batch_images = val_batch_images.to(device)\n","            val_out, val_latent_loss = vae_model(val_batch_images)\n","            val_recon_loss = criterion(val_out, val_batch_images)\n","            val_latent_loss = val_latent_loss.mean()\n","\n","            val_total_loss_batch =  val_recon_loss + beta * val_latent_loss\n","\n","            # Optimize\n","            val_total_loss_batch.backward()\n","            optimizer.step()\n","            val_total_loss_epoch += val_total_loss_batch.item() * val_batch_images.shape[0]\n","\n","\n","        val_log_info = (epoch + 1, epochs,  val_loss_rec_epoch/val_no_images, val_loss_KL_epoch/val_no_images, val_total_loss_epoch/val_no_images)\n","        log('%d/%d  Validation: Reconstruction Loss %.3f| KL Loss %.3f | Total Loss %.3f'% val_log_info)\n","        val_batch_avg_total_loss_list.append(val_total_loss_epoch/val_no_images)\n","        epoch_log_df = pd.DataFrame({\"train_loss\": train_batch_avg_total_loss_list, \"val_loss\": val_batch_avg_total_loss_list})\n","        print(epoch_log_df)\n","        epoch_log_df.to_csv(os.path.join(dir_info,'model_vae_t1_prostate_2d_16.csv'))\n","        if min_valid_loss > val_total_loss_epoch/val_no_images:\n","          print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{val_total_loss_epoch/val_no_images:.6f}) \\t Saving The Model')\n","          min_valid_loss = val_total_loss_epoch/val_no_images\n","          # Saving State Dict\n","\n","        torch.save(vae_model, path2save.format(epoch+1))\n","        # write csv\n","        #writer = csv.writer(f)\n","        #writer.writerow([epoch + 1, '{:04f}'.format(val_total_loss_epoch/train_no_images), '{:04f}'.format(val_total_loss_epoch/val_no_images)])\n","    #f.close()"]},{"cell_type":"markdown","source":["## Bladder training"],"metadata":{"id":"n9bNqrOCTc0W"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uxLphIxU0nf","colab":{"base_uri":"https://localhost:8080/","height":755},"executionInfo":{"status":"error","timestamp":1693281722273,"user_tz":300,"elapsed":63262,"user":{"displayName":"Vi Ly","userId":"05880124386121639962"}},"outputId":"1fd576c9-478c-42c5-f6db-aaaa3ab74ded"},"outputs":[{"output_type":"stream","name":"stdout","text":[" GPU is activated\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-1eb2d7fd442d>:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  torch.nn.init.xavier_uniform(m.weight.data)\n"]},{"output_type":"stream","name":"stdout","text":["Number of train fm images:  5000\n","Number of val fm images:  664\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/100 [00:00<?, ?it/s]\n","0it [00:00, ?it/s]\u001b[A<ipython-input-6-b70cece9af57>:40: DeprecationWarning: The *random* parameter to shuffle() has been deprecated\n","since Python 3.9 and will be removed in a subsequent version.\n","  random.shuffle(filenames, random.random)\n","\n","1it [00:07,  7.12s/it]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Encoding bottom\n","torch.Size([10, 128, 64, 64])\n","Encoding top\n","torch.Size([10, 128, 32, 32])\n"]},{"output_type":"stream","name":"stderr","text":["\n","2it [00:14,  7.18s/it]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Encoding bottom\n","torch.Size([10, 128, 64, 64])\n","Encoding top\n","torch.Size([10, 128, 32, 32])\n"]},{"output_type":"stream","name":"stderr","text":["2it [00:20, 10.09s/it]\n","  0%|          | 0/100 [00:20<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-250b58a65aa1>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mvae_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_batch_images\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m#train_batch_images = torch.concat([train_batch_images, train_batch_images, train_batch_images], dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-b70cece9af57>\u001b[0m in \u001b[0;36mload_data_images\u001b[0;34m(path, batch_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[1, ...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop_around_centroid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0m_ZIP_SUFFIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'PK\\x05\\x06'\u001b[0m \u001b[0;31m# empty zip files start with this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0;31m# If the file size is less than N, we need to make sure not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;31m# to seek past the beginning of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\"\"\"\n","Initialize Hyperparameters\n","\"\"\"\n","#import nibabel as ni\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os, glob\n","import torch\n","import csv\n","from tqdm import tqdm\n","##---------Settings--------------------------\n","batch_size = 10\n","lrate = 0.001\n","epochs = 100\n","weight_decay = 1e-6 #5e-7\n","\n","##############\n","train_path_data = \"/content/drive/MyDrive/VAE_GMM/2D-featuremap/Bladder/Bladder_train/sample\"\n","val_path_data =\"/content/drive/MyDrive/VAE_GMM/2D-featuremap/Bladder/Bladder_val\"\n","path2save = \"/content/drive/MyDrive/VAE_GMM/VQ-VAE2/vqvae2/vqvae2_checkpoint/bladder/model_vae_epoch_{}.pt\"\n","dir_info = '/content/drive/MyDrive/VAE_GMM/infor'\n","#f = open(os.path.join(dir_info,'model_vae_t1_rectum_2d.csv'),'w',newline='')\n","\n","####################\n","verbose = True\n","log = print if verbose else lambda *x, **i: None\n","np.random.seed(10)\n","torch.manual_seed(10)\n","###################\n","criterion = nn.MSELoss()\n","\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\" GPU is activated\" if device else \" CPU is activated\")\n","train_no_images = len(glob.glob(train_path_data + \"/*.npy\"))\n","val_no_images = len(glob.glob(val_path_data + \"/*.npy\"))\n","print(\"Number of train fm images: \", train_no_images)\n","print(\"Number of val fm images: \", val_no_images)\n","\n","if __name__==\"__main__\":\n","    vae_model = VQVAE()\n","    #ckp_path = \"/content/drive/MyDrive/VAE_GMM/VQ-VAE2/vqvae2/vqvae_560.pt\"\n","    #checkpoint =  torch.load(ckp_path)\n","    #vae_model.load_state_dict(checkpoint)\n","    vae_model.to(device)\n","    optimizer = torch.optim.Adam(vae_model.parameters(), lr=lrate, weight_decay=weight_decay)\n","\n","    vae_model.apply(weights_init)\n","    #log(vae_model)\n","    #optimizer = torch.optim.Adam(vae_model.parameters(), lr=lrate, weight_decay=weight_decay)\n","    min_valid_loss = np.inf\n","    #model, optimizer, start_epoch = load_ckp(ckp_path, vae_model, optimizer)\n","    train_batch_avg_total_loss_list = []\n","    val_batch_avg_total_loss_list = []\n","    beta = 0.25\n","\n","    for epoch in tqdm(range(epochs)):\n","        # training phrase\n","        train_loss_rec_batch, train_loss_KL_batch, train_total_loss_batch = 0, 0, 0\n","        train_loss_rec_epoch, train_loss_KL_epoch, train_total_loss_epoch = 0, 0, 0\n","\n","        vae_model.train()\n","        for train_batch_images in tqdm(load_data_images(train_path_data, batch_size)):\n","            optimizer.zero_grad()\n","            #train_batch_images = torch.concat([train_batch_images, train_batch_images, train_batch_images], dim=1)\n","            train_batch_images = train_batch_images.to(device)\n","            '''\n","            plt.imshow(train_batch_images[0, 0, ...].cpu().detach().numpy())\n","            plt.show()\n","            '''\n","            train_out, train_latent_loss = vae_model(train_batch_images)\n","            '''\n","            plt.imshow(train_out[0, 0, ...].cpu().detach().numpy())\n","            plt.show()\n","            '''\n","            train_recon_loss = criterion(train_out, train_batch_images)\n","            train_latent_loss = train_latent_loss.mean()\n","\n","            train_total_loss_batch =  train_recon_loss + beta * train_latent_loss\n","\n","            # Optimize\n","            train_total_loss_batch.backward()\n","            optimizer.step()\n","\n","            train_total_loss_epoch += train_total_loss_batch.item() * train_batch_images.shape[0]\n","\n","        train_log_info = (epoch + 1, epochs, train_loss_rec_epoch/train_no_images, train_loss_KL_epoch/train_no_images, train_total_loss_epoch/train_no_images)\n","        log('%d/%d  Train: Reconstruction Loss %.3f| KL Loss %.3f | Total Loss %.3f'% train_log_info)\n","        train_batch_avg_total_loss_list.append(train_total_loss_epoch/train_no_images)\n","\n","\n","        # validation phrase\n","        val_loss_rec_batch, val_loss_KL_batch, val_total_loss_batch = 0, 0, 0\n","        val_loss_rec_epoch, val_loss_KL_epoch, val_total_loss_epoch = 0, 0, 0\n","\n","\n","        vae_model.eval()\n","        for val_batch_images in tqdm(load_data_images(val_path_data, batch_size)):\n","            optimizer.zero_grad()\n","            #val_batch_images = torch.concat([val_batch_images, val_batch_images, val_batch_images], dim=1)\n","            val_batch_images = val_batch_images.to(device)\n","\n","            val_batch_images = val_batch_images.to(device)\n","            val_out, val_latent_loss = vae_model(val_batch_images)\n","            val_recon_loss = criterion(val_out, val_batch_images)\n","            val_latent_loss = val_latent_loss.mean()\n","\n","            val_total_loss_batch =  val_recon_loss + beta * val_latent_loss\n","\n","            # Optimize\n","            val_total_loss_batch.backward()\n","            optimizer.step()\n","            val_total_loss_epoch += val_total_loss_batch.item() * val_batch_images.shape[0]\n","\n","\n","        val_log_info = (epoch + 1, epochs,  val_loss_rec_epoch/val_no_images, val_loss_KL_epoch/val_no_images, val_total_loss_epoch/val_no_images)\n","        log('%d/%d  Validation: Reconstruction Loss %.3f| KL Loss %.3f | Total Loss %.3f'% val_log_info)\n","        val_batch_avg_total_loss_list.append(val_total_loss_epoch/val_no_images)\n","        epoch_log_df = pd.DataFrame({\"train_loss\": train_batch_avg_total_loss_list, \"val_loss\": val_batch_avg_total_loss_list})\n","        print(epoch_log_df)\n","        epoch_log_df.to_csv(os.path.join(dir_info,'model_vae_t1_prostate_2d_16.csv'))\n","        if min_valid_loss > val_total_loss_epoch/val_no_images:\n","          print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{val_total_loss_epoch/val_no_images:.6f}) \\t Saving The Model')\n","          min_valid_loss = val_total_loss_epoch/val_no_images\n","          # Saving State Dict\n","\n","        torch.save(vae_model, path2save.format(epoch+1))\n","        # write csv\n","        #writer = csv.writer(f)\n","        #writer.writerow([epoch + 1, '{:04f}'.format(val_total_loss_epoch/train_no_images), '{:04f}'.format(val_total_loss_epoch/val_no_images)])\n","    #f.close()"]}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNXW7H3j48iCHH6wpVQLDBA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}